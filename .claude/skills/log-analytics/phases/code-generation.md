# Phase 2: Code Generation

**Objective**: Write a Python script that parses the log data and extracts meaningful insights.

---

## üêç Generate Log Analysis Script

**CRITICAL**: Write code based on the ACTUAL log structure you saw in Phase 1.

**DO NOT use generic templates. Tailor the code to:**
- The specific fields present in the logs
- The error codes you identified
- The services that are logging
- The time range and patterns in the data

---

## üìù Script Requirements

Your generated script MUST:

### 1. **Parse the Actual Log Structure**

```python
import json
from datetime import datetime
from collections import Counter, defaultdict

def load_logs(filepath: str) -> dict:
    """Load logs from JSON file"""
    with open(filepath, 'r') as f:
        return json.load(f)

# Load the data structure you saved
data = load_logs('analytics/incident_logs.json')
logs = data['logs']  # Extract the logs array
```

### 2. **Count Error Patterns**

Analyze by:
- **Error type/code**: Which errors are most frequent?
- **Service**: Which services have the most errors?
- **Endpoint**: Which API endpoints are failing?
- **Time distribution**: When do errors spike?

```python
# Error analysis
error_logs = [log for log in logs if log['level'] == 'ERROR']
error_codes = Counter([log.get('error_code', 'UNKNOWN') for log in error_logs])
error_by_service = Counter([log['service'] for log in error_logs])
```

### 3. **Time-Series Analysis**

Group errors by hour/minute to detect spikes:

```python
errors_by_hour = defaultdict(int)
for log in error_logs:
    dt = datetime.fromisoformat(log['timestamp'])
    hour = dt.hour
    errors_by_hour[hour] += 1

peak_hour = max(errors_by_hour.items(), key=lambda x: x[1]) if errors_by_hour else None
```

### 4. **Performance Metrics**

Calculate response time statistics:

```python
response_times = [log['response_time_ms'] for log in logs if 'response_time_ms' in log]
response_times.sort()

def percentile(data, p):
    k = (len(data) - 1) * p
    f = int(k)
    c = k - f
    if f + 1 < len(data):
        return data[f] + (data[f + 1] - data[f]) * c
    return data[f]

p95 = percentile(response_times, 0.95)
p99 = percentile(response_times, 0.99)
avg = sum(response_times) / len(response_times)
```

### 5. **Detect Anomalies**

Identify unusual patterns:
- Services with abnormally high error rates
- Endpoints with response times > 5x average
- Sudden error spikes (hour-over-hour increase > 200%)

### 6. **Structured JSON Output**

Return results in a structured format:

```python
results = {
    "analysis_timestamp": datetime.now().isoformat(),
    "incident_id": data['incident_id'],
    "summary": {
        "total_logs": len(logs),
        "total_errors": len(error_logs),
        "error_rate_percent": round((len(error_logs) / len(logs)) * 100, 2)
    },
    "error_analysis": {
        "top_error_codes": error_codes.most_common(10),
        "errors_by_service": dict(error_by_service.most_common(10)),
        "peak_error_hour": peak_hour
    },
    "performance_metrics": {
        "avg_response_time_ms": round(avg, 2),
        "p95_response_time_ms": round(p95, 2),
        "p99_response_time_ms": round(p99, 2)
    },
    "anomalies": []  # Add detected anomalies here
}

print(json.dumps(results, indent=2))
```

---

## üíæ Save the Generated Script

**Use Write Tool**:

**CRITICAL - File Path MUST BE**: `analytics/parse_logs_[TIMESTAMP].py`

Example: `analytics/parse_logs_20251128_165500.py`

**üö® MANDATORY - FILE PATH MUST BE IN PROJECT DIRECTORY:**

```python
# ‚úÖ CORRECT - Use this EXACT pattern:
from datetime import datetime
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
script_path = f"analytics/parse_logs_{timestamp}.py"  # Relative path!

# ‚ùå FORBIDDEN - NEVER use these:
script_path = "/tmp/incident_log_analysis.py"        # WRONG!
script_path = "/private/tmp/analysis.py"             # WRONG!
script_path = "/tmp/parse_logs.py"                   # WRONG!
```

**Rules:**
- ‚úÖ MUST be relative path starting with `analytics/`
- ‚úÖ MUST include timestamp
- ‚ùå NO `/tmp/` - Files will be lost on reboot
- ‚ùå NO absolute paths outside project
- ‚ùå NO system directories

**Why timestamp?**
- Preserves version history
- Allows comparison of different analysis approaches
- Audit trail for incident investigation
- All analysis artifacts stay within secure project directory

**Verification:**
After saving, confirm the file path is: `/Users/<username>/CladeSkillDemo/analytics/parse_logs_*.py`
NOT `/tmp/` or `/private/tmp/`

---

## üé® Full Script Template

Here's the structure (customize based on YOUR log data):

```python
#!/usr/bin/env python3
"""
Log Analysis Script
Generated: [TIMESTAMP]
Incident: [INCIDENT_ID]
Purpose: Parse and analyze incident logs for error patterns and anomalies
"""

import json
from datetime import datetime
from collections import Counter, defaultdict


def load_logs(filepath: str) -> dict:
    """Load logs from JSON file"""
    with open(filepath, 'r') as f:
        return json.load(f)


def analyze_logs(data: dict) -> dict:
    """
    Main analysis function
    Customize this based on the actual log structure
    """
    logs = data['logs']
    
    # 1. Error Analysis
    error_logs = [log for log in logs if log['level'] == 'ERROR']
    error_codes = Counter([log.get('error_code', 'UNKNOWN') for log in error_logs])
    error_by_service = Counter([log['service'] for log in error_logs])
    
    # 2. Time-Series Analysis
    errors_by_hour = defaultdict(int)
    for log in error_logs:
        dt = datetime.fromisoformat(log['timestamp'])
        hour = dt.hour
        errors_by_hour[hour] += 1
    
    peak_hour = max(errors_by_hour.items(), key=lambda x: x[1]) if errors_by_hour else None
    
    # 3. Performance Metrics
    response_times = [log['response_time_ms'] for log in logs if 'response_time_ms' in log]
    response_times.sort()
    
    def percentile(data, p):
        if not data:
            return 0
        k = (len(data) - 1) * p
        f = int(k)
        c = k - f
        if f + 1 < len(data):
            return data[f] + (data[f + 1] - data[f]) * c
        return data[f]
    
    p95 = percentile(response_times, 0.95)
    p99 = percentile(response_times, 0.99)
    avg = sum(response_times) / len(response_times) if response_times else 0
    
    # 4. Anomaly Detection
    anomalies = []
    
    # Check for services with high error rates
    for service, count in error_by_service.items():
        service_logs = [log for log in logs if log['service'] == service]
        service_error_rate = (count / len(service_logs)) * 100 if service_logs else 0
        if service_error_rate > 5:  # > 5% error rate
            anomalies.append({
                "type": "high_error_rate",
                "service": service,
                "error_rate_percent": round(service_error_rate, 2),
                "severity": "high" if service_error_rate > 10 else "medium"
            })
    
    # 5. Build Results
    results = {
        "analysis_timestamp": datetime.now().isoformat(),
        "incident_id": data['incident_id'],
        "summary": {
            "total_logs": len(logs),
            "total_errors": len(error_logs),
            "total_warnings": len([log for log in logs if log['level'] == 'WARN']),
            "error_rate_percent": round((len(error_logs) / len(logs)) * 100, 2) if logs else 0
        },
        "error_analysis": {
            "top_error_codes": [{"code": code, "count": count} for code, count in error_codes.most_common(10)],
            "errors_by_service": dict(error_by_service.most_common(10)),
            "peak_error_hour": {"hour": peak_hour[0], "count": peak_hour[1]} if peak_hour else None,
            "hourly_distribution": dict(errors_by_hour)
        },
        "performance_metrics": {
            "avg_response_time_ms": round(avg, 2),
            "p95_response_time_ms": round(p95, 2),
            "p99_response_time_ms": round(p99, 2),
            "total_samples": len(response_times)
        },
        "anomalies": anomalies
    }
    
    return results


def main():
    """Main execution"""
    # Load data
    data = load_logs('analytics/incident_logs.json')
    
    # Analyze
    results = analyze_logs(data)
    
    # Output JSON
    print(json.dumps(results, indent=2))


if __name__ == "__main__":
    main()
```

---

## ‚úÖ Phase 2 Complete - Code Generation Summary

**Before proceeding, confirm:**
- ‚úÖ Python script generated and tailored to actual log structure
- ‚úÖ Script saved to `analytics/parse_logs_[TIMESTAMP].py`
- ‚úÖ Script includes: error counting, time-series analysis, performance metrics, anomaly detection
- ‚úÖ Script outputs structured JSON results
- ‚úÖ Script filename: [STATE THE FILENAME]

---

## Next Step

Once the analysis script is generated and saved, proceed to Phase 3: Analysis Execution.

**Read**: `phases/analysis-execution.md` to continue.

